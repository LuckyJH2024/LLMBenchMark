# ğŸ§  LLMBenchMark

This project benchmarks local Large Language Models (LLMs) on tasks such as **question answering**, **code generation**, **logical reasoning**, and **summarization**. It provides **structured evaluation results**, rich **visualizations**, and a detailed **markdown report**.

---

## ğŸš€ Installation and Environment Setup

### 1. Clone the project
```bash
git clone https://github.com/LuckyJH2024/LLMBenchMark.git your_file_name
cd your_file_name
```

### 2. Create and activate a virtual environment (optional)
```bash
python -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ§© Start Ollama and Load Models

### 4. Start Ollama
Ensure [Ollama](https://ollama.com/) is installed and running:
```bash
ollama serve
```

### 5. Pull required models
```bash
ollama run phi
ollama run mistral
ollama run llama3:8b
```

To check available models:
```bash
ollama list
```

---

## ğŸ§ª Run the Benchmark

### 6. Execute the main script
```bash
python main.py
```

This will:
- Run all task benchmarks
- Evaluate your selected models
- Generate outputs in `results/`:
  - `.json` results
  - Radar / bar / heatmap plots
  - Markdown summary report

---

## ğŸ“š Task Datasets

| Task Type      | File Name                  | Description                     |
|----------------|----------------------------|---------------------------------|
| QA             | `qa_benchmark.json`        | Question answering              |
| Code           | `code_benchmark.json`      | Code completion & generation    |
| Reasoning      | `sample_reasoning_eval.json` | Multi-type logical reasoning    |
| Summarization  | `summarization_benchmark.json` | Text summarization tasks    |

Modify or extend any dataset to suit your testing needs.

---

## ğŸ“Š View Results

- Markdown reports: `results/benchmark_report_*.md`
- Charts: `results/*.png`
- Viewable in VSCode, Typora, or any markdown/image viewer

---

## ğŸ’» Supported Models

You can use any model supported by Ollama, including:

- `phi`
- `mistral`
- `llama3:8b`

Update the model list in `main.py` to benchmark others.

---

## ğŸŒ Using Cloud API Models (NEW!)

In addition to local Ollama models, you can now benchmark cloud-based API models from:

- OpenAI (GPT-3.5, GPT-4)
- DeepSeek
- Anthropic (Claude)

### Setup API Keys

Set environment variables for the providers you want to use:

```bash
export OPENAI_API_KEY="your_openai_key"
export DEEPSEEK_API_KEY="your_deepseek_key"
export ANTHROPIC_API_KEY="your_anthropic_key"
```

### Run API benchmarks

```bash
python run_api_benchmark.py --models openai:gpt-4 anthropic:claude-3-sonnet
```

Additional options:
```bash
# Run specific task types
python run_api_benchmark.py --models openai:gpt-4 --task-types qa reasoning

# Change output directory
python run_api_benchmark.py --models deepseek:deepseek-chat --results-dir api_results
```

Results will be saved to the specified directory, with reports and visualizations similar to local model benchmarks.

---

## ğŸ§± Project Structure

```
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ run_api_benchmark.py    # API models benchmark script
â”œâ”€â”€ benchmark_framework/
â”‚   â”œâ”€â”€ benchmark.py        # Core logic & evaluation
â”‚   â”œâ”€â”€ api_models.py       # API model integration
â”‚   â”œâ”€â”€ tasks.py            # Load and format task data
â”‚   â”œâ”€â”€ visualization.py    # Create plots & dashboards
â”‚   â””â”€â”€ report.py           # Markdown report generation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/*.json             # Task definitions
â””â”€â”€ results/                # Output results and visualizations
```

---

# ğŸ§  Reasoning Benchmark Overview

### ğŸ§© File:
```
data/sample_reasoning_eval.json
```

### ğŸ§ª Structure:
```json
{
  "context": "...",
  "question": "...",
  "response": "...",
  "ground_truth": "...",
  "reasoning_steps": [...],
  "reference_steps": [...],
  "paraphrased_response": "..."
}
```

### ğŸ” Covered Reasoning Types

| Type                       | Description                                 | #
|---------------------------|---------------------------------------------|----|
| Multi-hop Reasoning       | Chain of facts (e.g., A > B > C)             | 3  |
| Syllogistic Reasoning     | Classic logic from category relationships   | 3  |
| Causal Reasoning          | Fallacy avoidance, cause-effect logic       | 3  |
| Numerical / Symbolic      | Basic arithmetic, quantities, logic math    | 3  |
| Boolean Reasoning         | Truth, contradiction, set membership        | 3  |
| Counterfactual Reasoning  | What-if reasoning                           | 3  |
| Planning / Procedural     | Goal-driven task planning                   | 3  |

---

### âš™ï¸ Evaluation Metrics

Each sample is automatically scored across:

| Metric             | Meaning                                              |
|--------------------|------------------------------------------------------|
| `answer_score`     | Response vs. ground truth (BERTScore / SBERT)       |
| `chain_score`      | Reasoning steps vs. reference reasoning             |
| `consistency_score`| Paraphrased response consistency                    |
| `score`            | Final weighted score (used in radar/visuals)        |

---

## ğŸ”¬ Three Evaluation Directions

### 1. âœ… Answer Accuracy Only
Evaluate whether the model gave the **correct final answer**.

- Compare `response` vs `ground_truth`
- Use BERTScore / SBERT
- Metric: `answer_score`

### 2. ğŸ”— Reasoning Process Matching
Compare **step-by-step** logic to `reference_steps`.

- Use `reasoning_steps` and `reference_steps`
- Metric: `chain_score` via average semantic match

### 3. â™»ï¸ Paraphrase Consistency
Check **robustness** against paraphrasing.

- Compare `response` and `paraphrased_response`
- Metric: `consistency_score`

---

## ğŸ“Œ Example Prompt Format

```
Context: All mammals are warm-blooded. Whales are mammals.
Question: Are whales warm-blooded?
Answer:
```

Expected output:
> Yes, because whales are mammals and all mammals are warm-blooded.

---

## ğŸ“¤ Extend the Benchmark

- Add new examples to `sample_reasoning_eval.json`
- Keep same field structure, append `"type"` as needed
- You may add adversarial variants for robustness testing

---

## ğŸ“ˆ Outputs and Visuals

- `results/{model}_reasoning.json`: Per-model scores
- `radar_chart.png`: 3-score profile per model
  ![Radar Chart](results/radar_chart.png)
- `performance_dashboard.png`: Model Ã— task comparison
  ![Performance Dashboard](results/performance_dashboard.png)
- `reasoning_bar_comparison.png`: Comparison of reasoning subscores (answer, chain, consistency) across models  
  ![Reasoning Subscore Bar Chart](results/reasoning_bar_comparison.png)

# å¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶

è¿™ä¸ªé¡¹ç›®æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰APIæ€§èƒ½çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚å®ƒæ”¯æŒå¤šç§ç±»å‹çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ¨ç†èƒ½åŠ›ã€ä»£ç ç”Ÿæˆä»¥åŠåŸºäºAPPSæ•°æ®é›†çš„ç¼–ç¨‹é¢˜è§£å†³èƒ½åŠ›æµ‹è¯•ã€‚

## åŠŸèƒ½ç‰¹ç‚¹

- æ”¯æŒOpenAIã€Anthropicã€Googleã€Baiduã€Zhipuç­‰å¤šç§APIæ¨¡å‹
- å†…ç½®å¤šç§è¯„ä¼°ä»»åŠ¡ï¼šæ¨ç†è¯„ä¼°ã€ç¼–ç èƒ½åŠ›æµ‹è¯•ã€APPSç¼–ç¨‹é¢˜ç›®æµ‹è¯•
- è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½è¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–ç»“æœ
- æ”¯æŒå¹¶è¡Œå¤„ç†ä»¥æé«˜æµ‹è¯•æ•ˆç‡
- æ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ‰©å±•å’Œå®šåˆ¶

## å®‰è£…ä¸é…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### 2. APIå¯†é’¥é…ç½®

ç³»ç»Ÿæ”¯æŒå¤šç§APIæä¾›å•†ï¼Œæ‚¨éœ€è¦åœ¨`config.json`æ–‡ä»¶ä¸­é…ç½®å¯¹åº”çš„APIå¯†é’¥ï¼š

```json
{
    "api_keys": {
        "openai": {
            "api_key": "YOUR_OPENAI_API_KEY"
        },
        "anthropic": {
            "api_key": "YOUR_ANTHROPIC_API_KEY"
        },
        "google": {
            "api_key": "YOUR_GOOGLE_API_KEY"
        },
        "baidu": {
            "api_key": "YOUR_BAIDU_API_KEY",
            "secret_key": "YOUR_BAIDU_SECRET_KEY"
        },
        "zhipu": {
            "api_key": "YOUR_ZHIPU_API_KEY"
        }
    },
    "api_base": {
        "openai": "https://api.openai.com/v1",
        "anthropic": "https://api.anthropic.com",
        "google": "https://generativelanguage.googleapis.com/v1",
        "baidu": "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop",
        "zhipu": "https://open.bigmodel.cn/api/paas"
    }
}
```

å¯¹äºéœ€è¦ä½¿ç”¨AWS Bedrockçš„Anthropic Claudeæ¨¡å‹ï¼Œé…ç½®å¦‚ä¸‹ï¼š

```json
{
    "api_keys": {
        "anthropic_bedrock": {
            "aws_access_key": "YOUR_AWS_ACCESS_KEY",
            "aws_secret_key": "YOUR_AWS_SECRET_KEY",
            "aws_region": "us-west-2"
        }
    }
}
```

å¯¹äºéœ€è¦ä½¿ç”¨Google Vertex AIçš„Claudeæ¨¡å‹ï¼Œé…ç½®å¦‚ä¸‹ï¼š

```json
{
    "api_keys": {
        "anthropic_vertex": {
            "project_id": "YOUR_GCP_PROJECT_ID",
            "region": "us-central1"
        }
    }
}
```

æ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥è®¾ç½®é…ç½®æ–‡ä»¶ï¼š

```bash
python setup_config.py
```

è¿™ä¸ªè„šæœ¬ä¼šå¼•å¯¼æ‚¨è¾“å…¥å„ä¸ªå¹³å°çš„APIå¯†é’¥ï¼Œå¹¶è‡ªåŠ¨åˆ›å»ºæˆ–æ›´æ–°é…ç½®æ–‡ä»¶ã€‚

## è¿è¡Œè¯„ä¼°

### 1. è¿è¡Œæ¨ç†èƒ½åŠ›è¯„ä¼°

```bash
python run_api_benchmark.py --models openai:gpt-4 anthropic:claude-3-5-sonnet-20240620 --tasks reasoning
```

### 2. è¿è¡Œç¼–ç èƒ½åŠ›è¯„ä¼°

```bash
python run_api_benchmark.py --models openai:gpt-4 anthropic:claude-3-5-sonnet-20240620 --tasks coding
```

### 3. è¿è¡ŒAPPSç¼–ç¨‹é¢˜è¯„ä¼°

```bash
python apps_benchmark.py --models openai:gpt-4 anthropic:claude-3-5-sonnet-20240620 --difficulties interview competition --problems 5
```

## ç”Ÿæˆå¯è§†åŒ–ç»“æœ

è¯„ä¼°ç»“æœä¼šè‡ªåŠ¨ä¿å­˜åˆ°`results`ç›®å½•ã€‚æ‚¨å¯ä»¥ç”Ÿæˆæ›´è¯¦ç»†çš„å¯è§†åŒ–ï¼š

```bash
python generate_heatmap.py
```

## æ”¯æŒçš„æ¨¡å‹

å½“å‰æ”¯æŒçš„æ¨¡å‹åŒ…æ‹¬ï¼š

| æä¾›å•† | æ¨¡å‹IDæ ¼å¼ | ç¤ºä¾‹ |
|--------|------------|------|
| OpenAI | openai:æ¨¡å‹å | openai:gpt-4 |
| Anthropic | anthropic:æ¨¡å‹å | anthropic:claude-3-5-sonnet-20240620 |
| Anthropic (Bedrock) | anthropic_bedrock:æ¨¡å‹å | anthropic_bedrock:anthropic.claude-3-5-sonnet-20240620-v1:0 |
| Anthropic (Vertex) | anthropic_vertex:æ¨¡å‹å | anthropic_vertex:claude-3-5-sonnet@20240620 |
| Google | google:æ¨¡å‹å | google:gemini-pro |
| Baidu | baidu:æ¨¡å‹å | baidu:ernie-4.0 |
| Zhipu | zhipu:æ¨¡å‹å | zhipu:glm-4 |

## è®¾ç½®è„šæœ¬ä½¿ç”¨è¯´æ˜

`setup_config.py`æ˜¯ä¸€ä¸ªäº¤äº’å¼è„šæœ¬ï¼Œå¯å¸®åŠ©æ‚¨è®¾ç½®APIå¯†é’¥ï¼š

```bash
python setup_config.py
```

æ‰§è¡Œåï¼Œè„šæœ¬ä¼šï¼š

1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰çš„é…ç½®æ–‡ä»¶
2. å¼•å¯¼æ‚¨é€‰æ‹©è¦é…ç½®çš„APIæä¾›å•†
3. æ ¹æ®é€‰æ‹©çš„æä¾›å•†ï¼Œæç¤ºè¾“å…¥ç›¸åº”çš„APIå¯†é’¥å’Œå…¶ä»–å¿…è¦ä¿¡æ¯
4. æ›´æ–°æˆ–åˆ›å»ºé…ç½®æ–‡ä»¶

ç¤ºä¾‹ï¼š

```
é€‰æ‹©è¦é…ç½®çš„APIæä¾›å•†:
1. OpenAI
2. Anthropic
3. Anthropic (Bedrock)
4. Anthropic (Vertex)
5. Google
6. Baidu
7. Zhipu
8. å…¨éƒ¨é…ç½®
9. é€€å‡º

è¯·è¾“å…¥é€‰é¡¹(1-9): 2
è¯·è¾“å…¥Anthropic APIå¯†é’¥: sk-ant-xxxxx
Anthropic APIå¯†é’¥å·²ä¿å­˜!
```

## æ³¨æ„äº‹é¡¹

- éƒ¨åˆ†APIæä¾›å•†å¯èƒ½éœ€è¦ç§‘å­¦ä¸Šç½‘æ‰èƒ½è®¿é—®
- è¯·ç¡®ä¿æ‚¨çš„APIå¯†é’¥æœ‰è¶³å¤Ÿçš„é¢åº¦æ¥å®Œæˆæµ‹è¯•
- å»ºè®®ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒæ¥é¿å…ä¾èµ–å†²çª
- APPSæ•°æ®é›†æµ‹è¯•éœ€è¦ä¸‹è½½å¹¶å°†æ•°æ®é›†æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„`APPS`æ–‡ä»¶å¤¹ä¸­